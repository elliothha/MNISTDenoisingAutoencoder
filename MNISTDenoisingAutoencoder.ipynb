{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84947fac-1757-4f0c-ad20-af37b05f6f0f",
   "metadata": {},
   "source": [
    "# Problem 2: Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec90ae-a410-47d4-9046-f51a39bc90fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install imagecorruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e1c0b-f78e-4462-8714-4efd580c24d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imagecorruptions import corrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a00d05-f2df-4eef-af4d-33e176c66a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMNIST(Dataset):\n",
    "    def __init__(self, mnist_dataset, corruption):\n",
    "        # Inputs: \n",
    "        # MNIST dataset\n",
    "        # Imagecorruptions corrupt function\n",
    "        \n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.corruption = corruption\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Outputs:\n",
    "        # returns the number of examples that we have\n",
    "        \n",
    "        return len(self.mnist_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Inputs:\n",
    "        # idx is the index of the example the dataloader is loading\n",
    "        # Outputs:\n",
    "        # (X_corrupted, X_original) = (corrupted version of original MNIST image, original)\n",
    "        # both are tensors of shape Channels x Height x Width\n",
    "        # the dataloader adds the extra dimension of batch_size automatically\n",
    "\n",
    "        image, _ = self.mnist_dataset[idx]\n",
    "\n",
    "        # Pad the image tensor to get to 32x32\n",
    "        padded_image = F.pad(image, (2, 2, 2, 2), 'constant', 0)\n",
    "\n",
    "        # Denormalize the image tensor to [0, 1]\n",
    "        denormalized_image = (padded_image + 1) / 2.0  # Assuming the tensor is normalized to [-1, 1]\n",
    "\n",
    "        # Convert the tensor values from [0, 1] to [0, 255] and then to a numpy array\n",
    "        image_np = (denormalized_image.squeeze(0).cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "        # Corrupt the image\n",
    "        corrupted_image_np = corrupt(image_np, corruption_name=self.corruption, severity=1)\n",
    "        corrupted_image_np = corrupted_image_np[:, :, 0]  # Take only one channel to make it grayscale\n",
    "\n",
    "        # Convert the numpy arrays back to torch tensors\n",
    "        X_original = transforms.ToTensor()(image_np).float()\n",
    "        X_corrupted = transforms.ToTensor()(corrupted_image_np).float()\n",
    "\n",
    "        return (X_corrupted, X_original)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b529646-1227-4321-89cf-a1c4a54ac15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('./data', download=True, train=False, transform=transform)\n",
    "\n",
    "# 80-20 split for training and validation\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Adding the corrupted image version to datasets\n",
    "corruption = 'gaussian_blur'\n",
    "trainset = CustomMNIST(trainset, corruption)\n",
    "valset = CustomMNIST(valset, corruption)\n",
    "testset = CustomMNIST(testset, corruption)\n",
    "\n",
    "batch_size = 8\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ef33b-1482-45a4-9939-0700f57f0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        # Input = Batch Size x Channels X Height x Width\n",
    "        \n",
    "        # Encoder Layer  = Input Size  -> Output Size\n",
    "        # First Conv     = BSx1x32x32  -> BSx32x32x32\n",
    "        # First Maxpool  = BSx32x32x32 -> BSx32x16x16\n",
    "        # Second Conv    = BSx32X16x16 -> BSx64x16x16\n",
    "        # Second Maxpool = BSx64x16x16 -> BSx64x8x8\n",
    "        # Third Conv     = BSx64x8x8   -> BSx128x8x8\n",
    "        # Third Maxpool  = BSx128x8x8  -> BSx128x4x4\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # O = [BS, 32, 32, 32]\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # O = [BS, 32, 16, 16]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # O = [BS, 64, 16, 16]\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # O = [BS, 64, 8, 8]\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # O = [BS, 128, 8, 8]\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2)  # O = [BS, 128, 4, 4]\n",
    "        )\n",
    "\n",
    "        # Decoder Layer   = Input Size  -> Output Size\n",
    "        # First Conv      = BSx128x4x4  -> BSx128x4x4\n",
    "        # First Upsample  = BSx128x4x4  -> BSx128x8x8\n",
    "        # Second Conv     = BSx128x8x8  -> BSx64x8x8\n",
    "        # Second Upsample = BSx64x8x8   -> BSx64x16x16\n",
    "        # Third Conv      = BSx64x16x16 -> BSx32x16x16\n",
    "        # Third Upsample  = BSx32x16x16 -> BSx32x32x32\n",
    "        # Fourth Conv     = BSx32x32x32 -> BSx1x32x32\n",
    "        # Sigmoid         = BSx1x32x32  -> BSx1x32x32\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),  # [BS, 128, 4, 4]\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2),  # [BS, 128, 8, 8]\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),  # [BS, 64, 8, 8]\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2),  # [BS, 64, 16, 16]\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),  # [BS, 32, 16, 16]\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2),  # [BS, 32, 32, 32]\n",
    "            nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=1),  # [BS, 1, 32, 32]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a5200-362b-43be-a148-ae0a704d94e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, trainloader, criterion, optimizer):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for corrupted, original in trainloader: \n",
    "        corrupted, original = corrupted.to(device), original.to(device)\n",
    "        \n",
    "        # Forward Pass\n",
    "        reconstructed = model(corrupted)\n",
    "        loss = criterion(reconstructed, original)\n",
    "        \n",
    "        # Backward Pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Average training loss for the epoch\n",
    "    train_loss /= len(trainloader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a64bc-b97c-4ea6-9444-3db409a50362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, valloader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # no need to track gradients\n",
    "        for corrupted, original in valloader:\n",
    "            corrupted, original = corrupted.to(device), original.to(device)\n",
    "            \n",
    "            reconstructed = model(corrupted)\n",
    "            loss = criterion(reconstructed, original)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Average validation loss\n",
    "    val_loss /= len(valloader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f00ba-7152-465a-bd6d-1f8b62c0566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "model = DenoisingAutoencoder()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd0f76-65ee-4a58-951e-91140fd14001",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(model, device, trainloader, criterion, optimizer)\n",
    "    val_loss = validate(model, device, valloader, criterion)\n",
    "    \n",
    "    training_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "    \n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Plotting\n",
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.plot(validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79635f3-65c6-44f7-85fe-b4fe825d1507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes tensors of 1x32x32\n",
    "# Plots 32x32 image\n",
    "def plotImages(corrupted, reconstructed, original):\n",
    "    corrupted = corrupted.cpu().detach().squeeze().numpy()\n",
    "    reconstructed = reconstructed.cpu().detach().squeeze().numpy()\n",
    "    original = original.cpu().detach().squeeze().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(corrupted, cmap='gray')\n",
    "    plt.title('Corrupted Image')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(reconstructed, cmap='gray')\n",
    "    plt.title('Reconstructed Image')\n",
    "    \n",
    "    plt.subplot(1, 3, 3) \n",
    "    plt.imshow(original, cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plotBatch(corrupted_batch, reconstructed_batch, original_batch):\n",
    "    batch_size = corrupted_batch.size(0)\n",
    "    for i in range(batch_size):\n",
    "        plotImages(corrupted_batch[i], reconstructed_batch[i], original_batch[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54266563-bcf6-4e2d-957b-75b2942c85d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, testloader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_losses = []\n",
    "    all_originals = []\n",
    "    all_corrupteds = []\n",
    "    all_reconstructed = []\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for corrupted, original in testloader:\n",
    "            corrupted, original = corrupted.to(device), original.to(device)\n",
    "            \n",
    "            reconstructed = model(corrupted)\n",
    "            loss = criterion(reconstructed, original)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Append data for plotting\n",
    "            all_losses.append(loss.item())\n",
    "            all_originals.append(original.cpu())\n",
    "            all_corrupteds.append(corrupted.cpu())\n",
    "            all_reconstructed.append(reconstructed.cpu())\n",
    "    \n",
    "    # Plot the first reconstruction from the first 10 batches\n",
    "    for loss, corrupted, reconstructed, original in zip(all_losses[:10], all_corrupteds[:10], all_reconstructed[:10], all_originals[:10]):\n",
    "        print(f\"Loss = {loss}\")\n",
    "        plotImages(corrupted[0], reconstructed[0], original[0])\n",
    "    \n",
    "    # Average test loss\n",
    "    test_loss /= len(testloader)\n",
    "    print(f\"Total Test Loss = {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b826f8-67e3-4139-8b11-517ad62f3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, device, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45903b1-2e54-4377-a25c-87883d98a761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
